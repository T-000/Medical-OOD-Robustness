#!/usr/bin/env python3
"""
KITS23 é¢„å¤„ç†å™¨ - æœ€ç»ˆç‰ˆæœ¬
ä½¿ç”¨ target_depth=128, ä¿æŒ512Ã—512åˆ†è¾¨ç‡
"""

import os
import glob
import nibabel as nib
import numpy as np
import torch
import torch.nn.functional as F

class KITS23Preprocessor:
    def __init__(self, target_depth=128, ct_min=-100, ct_max=400):
        """
        Args:
            target_depth: ç›®æ ‡æ·±åº¦ = 128åˆ‡ç‰‡
            ct_min: CTå€¼ä¸‹é™ = -100 (ä¿ç•™è‚¾è„ç»„ç»‡)
            ct_max: CTå€¼ä¸Šé™ = 400 (è£å‰ªé«˜å¯†åº¦åŒºåŸŸ)
        """
        self.target_depth = target_depth
        self.ct_min = ct_min
        self.ct_max = ct_max
        
        # å®˜æ–¹æ ‡ç­¾å®šä¹‰
        self.LABELS = {
            0: "background",
            1: "kidney", 
            2: "tumor",
            3: "cyst"
        }
    
    def detect_kidney_roi(self, segmentation_volume):
        """æ£€æµ‹è‚¾è„ROIåŒºåŸŸï¼ˆåŒ…æ‹¬è‚¾è„ã€è‚¿ç˜¤ã€å›Šè‚¿ï¼‰"""
        # è‚¾è„åŒºåŸŸ = è‚¾è„ + è‚¿ç˜¤ + å›Šè‚¿
        kidney_region = (segmentation_volume == 1) | (segmentation_volume == 2) | (segmentation_volume == 3)
        
        # æ‰¾åˆ°åŒ…å«è‚¾è„åŒºåŸŸçš„åˆ‡ç‰‡
        kidney_slices = np.any(kidney_region, axis=(1, 2))
        kidney_indices = np.where(kidney_slices)[0]
        
        if len(kidney_indices) == 0:
            print("Warning: No kidney region found in volume")
            return None
        
        # è®¡ç®—ROIè¾¹ç•Œï¼ˆå¢åŠ 10åˆ‡ç‰‡ç¼“å†²ï¼‰
        buffer = 10
        start_slice = max(0, kidney_indices[0] - buffer)
        end_slice = min(segmentation_volume.shape[0], kidney_indices[-1] + buffer + 1)
        
        print(f"è‚¾è„åŒºåŸŸ: åˆ‡ç‰‡ [{kidney_indices[0]}-{kidney_indices[-1]}]")
        print(f"ROIè£å‰ª: [{start_slice}-{end_slice}] (+{buffer}åˆ‡ç‰‡ç¼“å†²)")
        
        return start_slice, end_slice
    
    def normalize_ct(self, volume):
        """CTå€¼æ ‡å‡†åŒ–åˆ° [0, 1]"""
        volume = np.clip(volume, self.ct_min, self.ct_max)
        volume = (volume - self.ct_min) / (self.ct_max - self.ct_min)
        return volume
    
    def resize_depth(self, volume, target_depth, is_segmentation=False):
        """è°ƒæ•´æ·±åº¦ç»´åº¦åˆ°128åˆ‡ç‰‡ï¼Œä¿æŒ512Ã—512åˆ†è¾¨ç‡"""
        current_depth = volume.shape[0]
        
        if current_depth == target_depth:
            return volume
        
        print(f"æ·±åº¦è°ƒæ•´: {current_depth} -> {target_depth} åˆ‡ç‰‡")
        
        # é€‰æ‹©æ’å€¼æ–¹æ³•
        mode = 'nearest' if is_segmentation else 'trilinear'
        
        # è½¬æ¢ä¸ºTensor: [D, H, W] -> [1, 1, D, H, W]
        volume_tensor = torch.from_numpy(volume.astype(np.float32))
        volume_tensor = volume_tensor.unsqueeze(0).unsqueeze(0)
        
        # è°ƒæ•´æ·±åº¦ï¼Œä¿æŒ512Ã—512ä¸å˜
        resized = F.interpolate(
            volume_tensor, 
            size=(target_depth, 512, 512),  # ä¿æŒ512Ã—512åˆ†è¾¨ç‡!
            mode=mode,
            align_corners=False if mode == 'trilinear' else None
        )
        
        return resized.squeeze().numpy()
    
    def preprocess(self, imaging_path, segmentation_path):
        """å®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        print(f"\nğŸ“ å¤„ç†ç—…ä¾‹: {os.path.basename(os.path.dirname(imaging_path))}")
        print("-" * 50)
        
        # 1. åŠ è½½æ•°æ®
        imaging = nib.load(imaging_path)
        segmentation = nib.load(segmentation_path)
        
        imaging_data = imaging.get_fdata()
        segmentation_data = segmentation.get_fdata()
        
        print(f"åŸå§‹æ•°æ®:")
        print(f"  å½±åƒ: {imaging_data.shape} (èŒƒå›´: [{imaging_data.min():.0f}, {imaging_data.max():.0f}])")
        print(f"  åˆ†å‰²: {segmentation_data.shape}")
        
        # 2. æ£€æµ‹è‚¾è„ROIå¹¶è£å‰ª
        roi = self.detect_kidney_roi(segmentation_data)
        
        if roi:
            start, end = roi
            imaging_roi = imaging_data[start:end, :, :]
            segmentation_roi = segmentation_data[start:end, :, :]
            print(f"ROIè£å‰ªå: {imaging_roi.shape}")
        else:
            imaging_roi = imaging_data
            segmentation_roi = segmentation_data
            print("ä½¿ç”¨å®Œæ•´ä½“ç§¯")
        
        # 3. è°ƒæ•´æ·±åº¦åˆ°128åˆ‡ç‰‡
        imaging_roi = self.resize_depth(imaging_roi, self.target_depth, is_segmentation=False)
        segmentation_roi = self.resize_depth(segmentation_roi, self.target_depth, is_segmentation=True)
        
        # 4. CTå€¼æ ‡å‡†åŒ–
        imaging_roi = self.normalize_ct(imaging_roi)
        
        # 5. è½¬æ¢ä¸ºTensor
        imaging_tensor = torch.from_numpy(imaging_roi.astype(np.float32)).unsqueeze(0)  # [1, D, H, W]
        segmentation_tensor = torch.from_numpy(segmentation_roi.astype(np.int64)).unsqueeze(0)
        
        print(f"æœ€ç»ˆç»“æœ:")
        print(f"  å½±åƒ: {imaging_tensor.shape} (èŒƒå›´: [{imaging_tensor.min():.3f}, {imaging_tensor.max():.3f}])")
        print(f"  åˆ†å‰²: {segmentation_tensor.shape} (æ ‡ç­¾: {torch.unique(segmentation_tensor).tolist()})")
        
        return imaging_tensor, segmentation_tensor

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ KITS23 é¢„å¤„ç†å™¨ - æœ€ç»ˆç‰ˆæœ¬")
    print("å‚æ•°: depth=128, åˆ†è¾¨ç‡=512Ã—512, CTèŒƒå›´=[-100,400]")
    print("=" * 60)
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = KITS23Preprocessor(target_depth=128)
    
    # åœ¨ dataset ç›®å½•ä¸­æŸ¥æ‰¾ç—…ä¾‹
    dataset_path = "dataset"
    cases = glob.glob(f"{dataset_path}/case_*/imaging.nii.gz", recursive=True)
    
    if not cases:
        print("âŒ åœ¨ dataset ç›®å½•ä¸­æ‰¾ä¸åˆ°ç—…ä¾‹")
        return
    
    print(f"æ‰¾åˆ° {len(cases)} ä¸ªç—…ä¾‹")
    
    # æµ‹è¯•å‰3ä¸ªç—…ä¾‹
    for case_path in cases[:3]:
        seg_path = case_path.replace("imaging.nii.gz", "segmentation.nii.gz")
        
        if os.path.exists(seg_path):
            try:
                image_tensor, seg_tensor = preprocessor.preprocess(case_path, seg_path)
                print("âœ… é¢„å¤„ç†æˆåŠŸ!\n")
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}\n")
        else:
            print(f"âŒ æ‰¾ä¸åˆ°åˆ†å‰²æ–‡ä»¶: {seg_path}\n")

if __name__ == "__main__":
    main()
